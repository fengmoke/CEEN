digraph {
	graph [size="37.05,37.05"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2815048051552 [label="
 (1, 6)" fillcolor=darkolivegreen1]
	2815047948544 [label=AddmmBackward0]
	2815047948832 -> 2815047948544
	2815048049072 [label="classifier.1.linear.bias
 (6)" fillcolor=lightblue]
	2815048049072 -> 2815047948832
	2815047948832 [label=AccumulateGrad]
	2815047948736 -> 2815047948544
	2815047948736 [label=ViewBackward0]
	2815047948688 -> 2815047948736
	2815047948688 [label=AvgPool2DBackward0]
	2815047948976 -> 2815047948688
	2815047948976 [label=ReluBackward0]
	2815047949072 -> 2815047948976
	2815047949072 [label=NativeBatchNormBackward0]
	2815047949168 -> 2815047949072
	2815047949168 [label=ConvolutionBackward0]
	2815047949360 -> 2815047949168
	2815047949360 [label=ReluBackward0]
	2815047949504 -> 2815047949360
	2815047949504 [label=NativeBatchNormBackward0]
	2815047949600 -> 2815047949504
	2815047949600 [label=ConvolutionBackward0]
	2815047949792 -> 2815047949600
	2815047949792 [label=ViewBackward0]
	2815047949936 -> 2815047949792
	2815047949936 [label=SelectBackward0]
	2815047950032 -> 2815047949936
	2815047950032 [label=AddBackward0]
	2815047950128 -> 2815047950032
	2815047950128 [label=ReluBackward0]
	2815047950272 -> 2815047950128
	2815047950272 [label=NativeBatchNormBackward0]
	2815047950368 -> 2815047950272
	2815047950368 [label=ConvolutionBackward0]
	2815047950560 -> 2815047950368
	2815047950560 [label=ReluBackward0]
	2815047950704 -> 2815047950560
	2815047950704 [label=NativeBatchNormBackward0]
	2815047950800 -> 2815047950704
	2815047950800 [label=ConvolutionBackward0]
	2815047950992 -> 2815047950800
	2815047950992 [label=ReluBackward0]
	2815047951136 -> 2815047950992
	2815047951136 [label=NativeBatchNormBackward0]
	2815047951232 -> 2815047951136
	2815047951232 [label=ConvolutionBackward0]
	2815047951424 -> 2815047951232
	2815047951424 [label=AddBackward0]
	2815047951616 -> 2815047951424
	2815047951616 [label=ReluBackward0]
	2815047951760 -> 2815047951616
	2815047951760 [label=NativeBatchNormBackward0]
	2815047951856 -> 2815047951760
	2815047951856 [label=ConvolutionBackward0]
	2815047952048 -> 2815047951856
	2815047952048 [label=ReluBackward0]
	2815047952192 -> 2815047952048
	2815047952192 [label=NativeBatchNormBackward0]
	2815047952288 -> 2815047952192
	2815047952288 [label=ConvolutionBackward0]
	2815047952480 -> 2815047952288
	2815047952480 [label=ReluBackward0]
	2815047952624 -> 2815047952480
	2815047952624 [label=NativeBatchNormBackward0]
	2815047952720 -> 2815047952624
	2815047952720 [label=ConvolutionBackward0]
	2815047952912 -> 2815047952720
	2815047952912 [label=MaxPool2DWithIndicesBackward0]
	2815047953104 -> 2815047952912
	2815047953104 [label=ReluBackward0]
	2815047953200 -> 2815047953104
	2815047953200 [label=NativeBatchNormBackward0]
	2815047953296 -> 2815047953200
	2815047953296 [label=ConvolutionBackward0]
	2815047953488 -> 2815047953296
	2815047922800 [label="blocks.0.0.layers.0.0.weight
 (32, 1, 7, 1)" fillcolor=lightblue]
	2815047922800 -> 2815047953488
	2815047953488 [label=AccumulateGrad]
	2815047953440 -> 2815047953296
	2815047923040 [label="blocks.0.0.layers.0.0.bias
 (32)" fillcolor=lightblue]
	2815047923040 -> 2815047953440
	2815047953440 [label=AccumulateGrad]
	2815047953248 -> 2815047953200
	2815047924080 [label="blocks.0.0.layers.0.1.weight
 (32)" fillcolor=lightblue]
	2815047924080 -> 2815047953248
	2815047953248 [label=AccumulateGrad]
	2815047953008 -> 2815047953200
	2815047924160 [label="blocks.0.0.layers.0.1.bias
 (32)" fillcolor=lightblue]
	2815047924160 -> 2815047953008
	2815047953008 [label=AccumulateGrad]
	2815047952864 -> 2815047952720
	2815048040512 [label="blocks.0.1.net.0.weight
 (32, 32, 3, 1)" fillcolor=lightblue]
	2815048040512 -> 2815047952864
	2815047952864 [label=AccumulateGrad]
	2815047952816 -> 2815047952720
	2815048040592 [label="blocks.0.1.net.0.bias
 (32)" fillcolor=lightblue]
	2815048040592 -> 2815047952816
	2815047952816 [label=AccumulateGrad]
	2815047952672 -> 2815047952624
	2815048040672 [label="blocks.0.1.net.1.weight
 (32)" fillcolor=lightblue]
	2815048040672 -> 2815047952672
	2815047952672 [label=AccumulateGrad]
	2815047952528 -> 2815047952624
	2815048040752 [label="blocks.0.1.net.1.bias
 (32)" fillcolor=lightblue]
	2815048040752 -> 2815047952528
	2815047952528 [label=AccumulateGrad]
	2815047952432 -> 2815047952288
	2815048042112 [label="blocks.0.2.net.0.weight
 (64, 32, 3, 1)" fillcolor=lightblue]
	2815048042112 -> 2815047952432
	2815047952432 [label=AccumulateGrad]
	2815047952240 -> 2815047952192
	2815048042192 [label="blocks.0.2.net.1.weight
 (64)" fillcolor=lightblue]
	2815048042192 -> 2815047952240
	2815047952240 [label=AccumulateGrad]
	2815047952096 -> 2815047952192
	2815048042272 [label="blocks.0.2.net.1.bias
 (64)" fillcolor=lightblue]
	2815048042272 -> 2815047952096
	2815047952096 [label=AccumulateGrad]
	2815047952000 -> 2815047951856
	2815048042672 [label="blocks.0.2.net.3.weight
 (64, 64, 3, 1)" fillcolor=lightblue]
	2815048042672 -> 2815047952000
	2815047952000 [label=AccumulateGrad]
	2815047951808 -> 2815047951760
	2815048042752 [label="blocks.0.2.net.4.weight
 (64)" fillcolor=lightblue]
	2815048042752 -> 2815047951808
	2815047951808 [label=AccumulateGrad]
	2815047951664 -> 2815047951760
	2815048042832 [label="blocks.0.2.net.4.bias
 (64)" fillcolor=lightblue]
	2815048042832 -> 2815047951664
	2815047951664 [label=AccumulateGrad]
	2815047951568 -> 2815047951424
	2815047951568 [label=ReluBackward0]
	2815047951952 -> 2815047951568
	2815047951952 [label=NativeBatchNormBackward0]
	2815047952336 -> 2815047951952
	2815047952336 [label=ConvolutionBackward0]
	2815047952480 -> 2815047952336
	2815047952576 -> 2815047952336
	2815048041312 [label="blocks.0.2.shortcut.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	2815048041312 -> 2815047952576
	2815047952576 [label=AccumulateGrad]
	2815047952768 -> 2815047952336
	2815048041392 [label="blocks.0.2.shortcut.bias
 (64)" fillcolor=lightblue]
	2815048041392 -> 2815047952768
	2815047952768 [label=AccumulateGrad]
	2815047952384 -> 2815047951952
	2815048041472 [label="blocks.0.2.bn.weight
 (64)" fillcolor=lightblue]
	2815048041472 -> 2815047952384
	2815047952384 [label=AccumulateGrad]
	2815047951712 -> 2815047951952
	2815048041552 [label="blocks.0.2.bn.bias
 (64)" fillcolor=lightblue]
	2815048041552 -> 2815047951712
	2815047951712 [label=AccumulateGrad]
	2815047951376 -> 2815047951232
	2815048045152 [label="blocks.1.0.net.0.weight
 (64, 64, 3, 1)" fillcolor=lightblue]
	2815048045152 -> 2815047951376
	2815047951376 [label=AccumulateGrad]
	2815047951328 -> 2815047951232
	2815048045232 [label="blocks.1.0.net.0.bias
 (64)" fillcolor=lightblue]
	2815048045232 -> 2815047951328
	2815047951328 [label=AccumulateGrad]
	2815047951184 -> 2815047951136
	2815048045312 [label="blocks.1.0.net.1.weight
 (64)" fillcolor=lightblue]
	2815048045312 -> 2815047951184
	2815047951184 [label=AccumulateGrad]
	2815047951040 -> 2815047951136
	2815048045392 [label="blocks.1.0.net.1.bias
 (64)" fillcolor=lightblue]
	2815048045392 -> 2815047951040
	2815047951040 [label=AccumulateGrad]
	2815047950944 -> 2815047950800
	2815048046592 [label="blocks.1.1.net.0.weight
 (128, 64, 3, 1)" fillcolor=lightblue]
	2815048046592 -> 2815047950944
	2815047950944 [label=AccumulateGrad]
	2815047950752 -> 2815047950704
	2815048046672 [label="blocks.1.1.net.1.weight
 (128)" fillcolor=lightblue]
	2815048046672 -> 2815047950752
	2815047950752 [label=AccumulateGrad]
	2815047950608 -> 2815047950704
	2815048046752 [label="blocks.1.1.net.1.bias
 (128)" fillcolor=lightblue]
	2815048046752 -> 2815047950608
	2815047950608 [label=AccumulateGrad]
	2815047950512 -> 2815047950368
	2815048047232 [label="blocks.1.1.net.3.weight
 (128, 128, 3, 1)" fillcolor=lightblue]
	2815048047232 -> 2815047950512
	2815047950512 [label=AccumulateGrad]
	2815047950320 -> 2815047950272
	2815048047312 [label="blocks.1.1.net.4.weight
 (128)" fillcolor=lightblue]
	2815048047312 -> 2815047950320
	2815047950320 [label=AccumulateGrad]
	2815047950176 -> 2815047950272
	2815048047392 [label="blocks.1.1.net.4.bias
 (128)" fillcolor=lightblue]
	2815048047392 -> 2815047950176
	2815047950176 [label=AccumulateGrad]
	2815047950080 -> 2815047950032
	2815047950080 [label=ReluBackward0]
	2815047950464 -> 2815047950080
	2815047950464 [label=NativeBatchNormBackward0]
	2815047950848 -> 2815047950464
	2815047950848 [label=ConvolutionBackward0]
	2815047950992 -> 2815047950848
	2815047951088 -> 2815047950848
	2815048045872 [label="blocks.1.1.shortcut.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2815048045872 -> 2815047951088
	2815047951088 [label=AccumulateGrad]
	2815047951280 -> 2815047950848
	2815048045952 [label="blocks.1.1.shortcut.bias
 (128)" fillcolor=lightblue]
	2815048045952 -> 2815047951280
	2815047951280 [label=AccumulateGrad]
	2815047950896 -> 2815047950464
	2815048046032 [label="blocks.1.1.bn.weight
 (128)" fillcolor=lightblue]
	2815048046032 -> 2815047950896
	2815047950896 [label=AccumulateGrad]
	2815047950224 -> 2815047950464
	2815048046112 [label="blocks.1.1.bn.bias
 (128)" fillcolor=lightblue]
	2815048046112 -> 2815047950224
	2815047950224 [label=AccumulateGrad]
	2815047949744 -> 2815047949600
	2815048047792 [label="classifier.1.m.0.net.0.weight
 (128, 128, 3, 1)" fillcolor=lightblue]
	2815048047792 -> 2815047949744
	2815047949744 [label=AccumulateGrad]
	2815047949552 -> 2815047949504
	2815048047872 [label="classifier.1.m.0.net.1.weight
 (128)" fillcolor=lightblue]
	2815048047872 -> 2815047949552
	2815047949552 [label=AccumulateGrad]
	2815047949408 -> 2815047949504
	2815048047952 [label="classifier.1.m.0.net.1.bias
 (128)" fillcolor=lightblue]
	2815048047952 -> 2815047949408
	2815047949408 [label=AccumulateGrad]
	2815047949312 -> 2815047949168
	2815048048432 [label="classifier.1.m.1.net.0.weight
 (128, 128, 3, 1)" fillcolor=lightblue]
	2815048048432 -> 2815047949312
	2815047949312 [label=AccumulateGrad]
	2815047949120 -> 2815047949072
	2815048048512 [label="classifier.1.m.1.net.1.weight
 (128)" fillcolor=lightblue]
	2815048048512 -> 2815047949120
	2815047949120 [label=AccumulateGrad]
	2815047948640 -> 2815047949072
	2815048048592 [label="classifier.1.m.1.net.1.bias
 (128)" fillcolor=lightblue]
	2815048048592 -> 2815047948640
	2815047948640 [label=AccumulateGrad]
	2815047948784 -> 2815047948544
	2815047948784 [label=TBackward0]
	2815047949024 -> 2815047948784
	2815048048992 [label="classifier.1.linear.weight
 (6, 384)" fillcolor=lightblue]
	2815048048992 -> 2815047949024
	2815047949024 [label=AccumulateGrad]
	2815047948544 -> 2815048051552
}
